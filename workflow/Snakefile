# Main entrypoint of the workflow.
# Please follow the best practices:
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there.

import functools
import pickle as pkl
import pandas as pd

from snakemake.utils import min_version

min_version("6.0")

configfile: workflow.source_path("config/config.yaml")

wildcard_constraints:
    n="\d+",
    mriot_name="EXIOBASE3-2010"

# rule get_hazard_dirimpact:
#     params:
#         hazfiles=[
#             (
#                 "tropical_cyclone",
#                 "tropical_cyclone_0synth_tracks_150arcsec_genesis_WP_1980_2020",
#             ),
#             (
#                 "tropical_cyclone",
#                 "tropical_cyclone_0synth_tracks_150arcsec_genesis_NA_1980_2020",
#             ),
#         ],
#         year_start=2012,
#         year_end=2017,
#     log:
#         "logs/get_hazard_dirimpact.log"
#     benchmark:
#         "benchmarks/get_hazard_dirimpact.csv"
#     resources:
#         mem_mb=10000
#     conda:
#         "envs/climada_env.yml"
#     output:
#         save_path="outputs/direct_impact.pkl",
#         assets="outputs/assets.pkl"
#     script:
#         "scripts/hazard.py"

rule impact_csv_csr_to_pkl:
    input:
        csv = "inputs/direct_impacts_yearsets/sample_{n}_5y_impact_na_wp.csv",
        npz = "inputs/direct_impacts_yearsets/sample_{n}_5y_impact_na_wp.npz",
        assets = "outputs/assets.pkl"
    output:
        pkl = "outputs/sample_{n}/direct_impact.pkl",
    log:
        "logs/yearsets_{n}_impacts_to_pkl.log"
    benchmark:
        "benchmarks/yearsets_{n}_impacts_to_pkl.csv"
    resources:
        mem_mb= 10000
    conda:
        "envs/climada_env.yml"
    script:
        "scripts/yearset_to_pkl.py"

checkpoint create_supchain:
    input:
        impact_file="outputs/sample_{n}/direct_impact.pkl",
        mriot_file="inputs/{mriot_name}/mriot_original.pkl",
        assets="outputs/assets.pkl",
    params:
        sector_common_aggreg="inputs/sectors_common_aggreg.ods",
        direct_dmg_floor=1e8,
    log:
        "logs/create_supchain/{n}_{mriot_name}.log"
    benchmark:
        "benchmarks/create_supchain/{n}_{mriot_name}.csv"
    resources:
        mem_mb=12000
    conda:
        "envs/climada_env.yml"
    output:
        save_path="outputs/sample_{n}/{mriot_name}_supchain.pkl",
        df_impact="outputs/sample_{n}/{mriot_name}_df_impact.parquet",
        mriot_save="outputs/sample_{n}/{mriot_name}.pkl"
    script:
        "scripts/supchain.py"


rule simulate:
    input:
        supchain="outputs/sample_{n}/{mriot_name}_supchain.pkl",
        mriot_file="outputs/sample_{n}/{mriot_name}.pkl",
        sectors_df="inputs/sectors_df.parquet",
        df_impact="outputs/sample_{n}/{mriot_name}_df_impact.parquet"
    params:
        rebuild_tau=365,
        duration=3,
        order_type="alt",
        alpha_tau=365,
        inv_tau=60,
        psi=0.80,
        monetary_factor=100000,
    wildcard_constraints:
        event_id="aggregated|\d+"
    log:
        "logs/simulate/{n}_{mriot_name}_{event_id}.log"
    benchmark:
        "benchmarks/simulate/{n}_{mriot_name}_{event_id}.csv"
    resources:
        mem_mb=6000
    conda:
        "envs/climada_env.yml"
    output:
        path=directory("outputs/simulations/sample_{n}/{mriot_name}_event_{event_id}"),
        results=expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_event_{{event_id}}/{variable}.parquet",
            variable=config["variables"],
        ),
    script:
        "scripts/indirect_impact.py"


def all_isolate_events(wildcards):
    df_impact = pd.read_parquet(checkpoints.create_supchain.get(**wildcards).output.df_impact).reset_index()
    return (
        expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_event_{event_id}/{variable}.parquet",
            variable=config["variables"],
            event_id=df_impact.index,
        )
    )

rule reduce:
    input:
        all_isolate_events
    output:
        expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_event_separated/{variable}.parquet",
            variable=config["variables"],
        ),
    resources:
        mem_mb=6000,
    log:
        "logs/reduce/{n}_{mriot_name}.log"
    benchmark:
        "benchmarks/reduce/{n}_{mriot_name}.log"
    run:
        structured_results = { var: [pd.read_parquet(isolate) for isolate in input if var in isolate] for var in config["variables"] }
        for var, group in structured_results.items():
            functools.reduce(
                lambda x, y: x.add(y - y.loc[0], fill_value=0), group
            ).to_parquet(
                f"outputs/simulations/sample_{wildcards.n}/{wildcards.mriot_name}_event_separated/{var}.parquet"
            )

def get_all_results(wildcards):
    n_samples = glob_wildcards(f"inputs/direct_impacts_yearsets/sample_{{n}}_5y_impact_na_wp.csv").n
    return expand("outputs/simulations/sample_{n}/{mriot}_event_{evtype}/{var}.parquet", n=n_samples, mriot="EXIOBASE3-2010", evtype=["aggregated","separated"], var=config["variables"])

rule all_results:
    input:
        get_all_results
