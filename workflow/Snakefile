# Main entrypoint of the workflow.
# Please follow the best practices:
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there.

import functools
import pickle as pkl
import pandas as pd

from snakemake.utils import min_version

min_version("6.0")

configfile: workflow.source_path("config/config.yaml")

wildcard_constraints:
    n="\d+",
    mriot_name="EXIOBASE3-2010|OECD21-2018"

rule impact_csv_csr_to_pkl:
    """Read a npz and csv couple and create an Impact object then saved using pickle"""
    input:
        csv = "inputs/direct_impacts_yearsets/sample_{n}_5y_impact_na_wp.csv",
        npz = "inputs/direct_impacts_yearsets/sample_{n}_5y_impact_na_wp.npz",
        assets = "outputs/assets.pkl"
    output:
        pkl = "outputs/sample_{n}/direct_impact.pkl",
    log:
        "logs/yearsets_{n}_impacts_to_pkl.log"
    # benchmark:
        # "benchmarks/yearsets_{n}_impacts_to_pkl.csv"
    resources:
        mem_mb= 10000
    conda:
        "envs/climada_env.yml"
    script:
        "scripts/yearset_to_pkl.py"

def get_all_impacts(wildcards):
    n_samples = glob_wildcards(f"inputs/direct_impacts_yearsets/sample_{{n}}_5y_impact_na_wp.csv").n
    return expand("outputs/sample_{n}/direct_impact.pkl", n=n_samples)

rule all_impact_pkl:
    input:
        get_all_impacts

checkpoint create_supchain:
    """Create a supply chain object (climada), an impact and meta dataframe as well as the IOSystem used.

    Supply chain contains the per region,sector direct impact for the given MRIOT for each events in the impact dataframe. region,sector impacts lower than 'direct_dmg_floor' (snakemake config) are ignored.

    df_impact contains all the retained events. Its index is the step of event occurrence.

    meta_df_impact contains a higher level set of information about the events (total damages, affected regions, shock intensity (as the maximum share of assets destroyed of all affected regions), recovery duration)
    """
    input:
        impact_file="outputs/sample_{n}/direct_impact.pkl",
        mriot_file="inputs/{mriot_name}/mriot_original.pkl",
        assets="outputs/assets.pkl",
    params:
        sector_common_aggreg="inputs/sectors_common_aggreg.ods",
        direct_dmg_floor=1e8,
    log:
        "logs/create_supchain/{n}_{mriot_name}.log"
    # benchmark:
        # "benchmarks/create_supchain/{n}_{mriot_name}.csv"
    resources:
        mem_mb=12000
    conda:
        "envs/climada_env.yml"
    output:
        save_path="outputs/sample_{n}/{mriot_name}_supchain.pkl",
        df_impact="outputs/sample_{n}/{mriot_name}_df_impact.parquet",
        meta_df_impact="outputs/sample_{n}/{mriot_name}_meta_df_impact.parquet",
        mriot_save="outputs/sample_{n}/{mriot_name}.pkl"
    script:
        "scripts/supchain.py"

def get_all_supchains(wildcards):
    n_samples = glob_wildcards(f"inputs/direct_impacts_yearsets/sample_{{n}}_5y_impact_na_wp.csv").n
    supchains = expand("outputs/sample_{n}/{mriot}_supchain.pkl", n=n_samples, mriot=config["MRIOT"])
    mriots = expand("outputs/sample_{n}/{mriot}_df_impact.parquet", n=n_samples, mriot=config["MRIOT"])
    df_impact = expand("outputs/sample_{n}/{mriot}_df_impact.parquet", n=n_samples, mriot=config["MRIOT"])
    meta_df_impact = expand("outputs/sample_{n}/{mriot}_meta_df_impact.parquet", n=n_samples, mriot=config["MRIOT"])
    return supchains + mriots + df_impact + meta_df_impact


rule all_supchains_pkl:
    input:
        get_all_supchains

rule simulate:
    """run a simulation for an event or a set of events"""
    input:
        supchain="outputs/sample_{n}/{mriot_name}_supchain.pkl",
        mriot_file="outputs/sample_{n}/{mriot_name}.pkl",
        sectors_df="inputs/{inv_sce}_sectors_df.parquet",
        df_impact="outputs/sample_{n}/{mriot_name}_df_impact.parquet",
        meta_df_impact="outputs/sample_{n}/{mriot_name}_meta_df_impact.parquet",
    params:
        rebuild_tau=config["rebuild_tau"],
        duration=config["duration"],
        order_type=config["order_type"],
        alpha_max=config["alpha_max"],
        alpha_tau=config["alpha_tau"],
        psi=config["psi"],
        monetary_factor=config["monetary_factor"],
        step_to_eq=config["step_to_eq"],
        variables=config["variables"]
    wildcard_constraints:
        event_id="aggregated|\d+"
    log:
        "logs/simulate/{n}_{mriot_name}_{inv_sce}_{event_id}.log"
    # benchmark:
        # "benchmarks/simulate/{n}_{mriot_name}_{inv_sce}_{event_id}.csv"
    resources:
        mem_mb=72000
    conda:
        "envs/climada_env.yml"
    output:
        path=directory("outputs/simulations/sample_{n}/{mriot_name}_{inv_sce}_event_{event_id}"),
        results=expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_{{inv_sce}}_event_{{event_id}}/{variable}.parquet",
            variable=config["variables"],
        ),
    script:
        "scripts/indirect_impact.py"


def all_isolate_events(wildcards):
    """return the list of parquet files of results for all variables for each isolated events run for a given sample, MRIOT, inventory scenario"""
    df_impact = pd.read_parquet(checkpoints.create_supchain.get(**wildcards).output.df_impact).reset_index()
    return (
        expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_{{inv_sce}}_event_{event_id}/{variable}.parquet",
            variable=config["variables"],
            event_id=df_impact.index,
        )
    )

rule reduce:
    """Produce the aggregation of all isolated events"""
    input:
        all_isolate_events
    output:
        expand(
            "outputs/simulations/sample_{{n}}/{{mriot_name}}_{{inv_sce}}_event_separated/{variable}.parquet",
            variable=config["variables"],
        ),
    resources:
        mem_mb=1000*len(config["variables"]),
    log:
        "logs/reduce/{n}_{mriot_name}_{inv_sce}.log"
    # benchmark:
        # "benchmarks/reduce/{n}_{mriot_name}_{inv_sce}.log"
    run:
        structured_results = { var: [pd.read_parquet(isolate) for isolate in input if var in isolate] for var in config["variables"] }
        for var, group in structured_results.items():
            functools.reduce(
                lambda x, y: x.add(y - y.loc[0], fill_value=0), group
            ).to_parquet(
                f"outputs/simulations/sample_{wildcards.n}/{wildcards.mriot_name}_{wildcards.inv_sce}_event_separated/{var}.parquet"
            )

def get_all_results(wildcards):
    n_samples = glob_wildcards(f"inputs/direct_impacts_yearsets/sample_{{n}}_5y_impact_na_wp.csv").n
    return expand("outputs/simulations/sample_{n}/{mriot}_{inv_sce}_event_{evtype}/{var}.parquet", n=n_samples, mriot=config["MRIOT"], inv_sce=config["inv_sce"], evtype=["aggregated","separated"], var=config["variables"])

def get_all_impacts_df(wildcards):
    n_samples = glob_wildcards(f"inputs/direct_impacts_yearsets/sample_{{n}}_5y_impact_na_wp.csv").n
    return expand("outputs/sample_{n}/{mriot}_df_impact.parquet", n=n_samples, mriot=config["MRIOT"])

rule all_results:
    input:
        get_all_results

rule meta_df:
    """Produce the file grouping all results"""
    input:
        results=get_all_results,
        impacts=get_all_impacts_df
    output:
        meta_results="outputs/results_meta_df.parquet"
    resources:
        mem_mb=6000,
    log:
        "logs/meta_df.log"
    # benchmark:
        # "benchmarks/meta_df.csv"
    script:
        "scripts/meta_df_building.py"
